# -*- coding: utf-8 -*-
"""Loan_Dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1t249RCjGXJkTAgxzyu-3WlPgqyGRfUG_
"""

import warnings
warnings.filterwarnings('ignore')

"""**Importing the libraries**"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

"""**Importing the dataset**"""

data = pd.read_excel('/content/loan_data_set k.xlsx')
data.head(10)

data.tail()

"""**From the below figure we can understand that there are more Yes values as compared to No values. We have to use SMOTE to balance the dataset.**"""

data['Loan_Status'].hist()

"""**To detect the missing values**"""

data.isnull().sum()

missing_data = data.isnull().sum()
total_percentage = (missing_data.sum()/data.shape[0]) * 100
print(f'The total percentage of missing data is {round(total_percentage,2)}%')

"""**Percentage of the missing values**"""

# percentage of missing data per category
total = data.isnull().sum().sort_values(ascending=False)
percent_total = (data.isnull().sum()/data.isnull().count()).sort_values(ascending=False)*100
missing = pd.concat([total, percent_total], axis=1, keys=["Total", "Percentage"])
missing_data = missing[missing['Total']>0]
missing_data

plt.figure(figsize=(9,6))
sns.set(style="whitegrid")
sns.barplot(x=missing_data.index, y=missing_data['Percentage'], data = missing_data)
plt.title('Percentage of missing data by feature')
plt.xlabel('Features', fontsize=14)
plt.ylabel('Percentage', fontsize=14)
plt.show()

data.shape

data.info()

"""**To check whether there are any duplicate values.**"""

data.duplicated().sum()

"""The data distribution of the variables

"""

fig = plt.figure(figsize = (15,20))
ax = fig.gca()
data.hist(ax = ax)
plt.show()

"""**The correlation matrix of the variables.**"""

plt.figure(figsize=(15,8))
sns.heatmap(data.corr(),annot=True)
plt.show()

data.describe()

data.head()

data['Gender'] = data['Gender'].map({'Male': 1, 'Female': 0})
data['Married'] = data['Married'].map({'Yes': 1, 'No': 0})
data['Education'] = data['Education'].map({'Graduate': 1, 'Not Graduate': 0})
data['Self_Employed'] = data['Self_Employed'].map({'Yes': 1, 'No': 0})
data['Loan_Status'] = data['Loan_Status'].map({'Y': 1, 'N': 0})

dependents = pd.get_dummies(data['Dependents'],prefix='Dep',drop_first=True)
data = pd.concat([data,dependents],axis=1)

property_area = pd.get_dummies(data['Property_Area'],prefix='Prop_area',drop_first=True)
data = pd.concat([data,property_area],axis=1)

data.shape

data.head(10)

data.drop(['Dependents'],axis=1,inplace=True)

data.drop(['Property_Area'],axis=1,inplace=True)

data.shape

data.head()

data.shape

data.drop(['Loan_ID'],axis=1,inplace=True)

data.dropna(axis=0, inplace=True)

data.isna().sum()

"""The boxplot will show us the outliers.

"""

sns.boxplot(x=data['ApplicantIncome'])

sns.boxplot(x=data['LoanAmount'])

sns.boxplot(x=data['CoapplicantIncome'])

"""**Since we don't see much outliers, it's better we don't remove the outliers as it may remove the relevant information.**"""

plt.figure(figsize=(20,10))
sns.heatmap(data.corr(),annot=True)

data.columns

"""**Now we will check which variables are significant for our study using Logit function**"""

X = data.drop(['Loan_Status'],1)
y = data['Loan_Status']

import statsmodels.api as sm

res = sm.Logit(y,X).fit()
res.summary()

X.drop(['Gender','ApplicantIncome','Dep_3+'],1,inplace=True)

import statsmodels.api as sm

res = sm.Logit(y,X).fit()
res.summary()

X.drop(['Self_Employed','Dep_2','Prop_area_Urban','Dep_1'],1,inplace=True)

import statsmodels.api as sm

res = sm.Logit(y,X).fit()
res.summary()

X.drop(['Education','CoapplicantIncome'],1,inplace=True)

import statsmodels.api as sm

res = sm.Logit(y,X).fit()
res.summary()

X.drop(['Married'],1,inplace=True)

import statsmodels.api as sm

res = sm.Logit(y,X).fit()
res.summary()

X.drop(['LoanAmount'],1,inplace=True)

import statsmodels.api as sm

res = sm.Logit(y,X).fit()
res.summary()

"""**Using the logit function we can see that we only have 3 relevant variables i.e. Loan amt term, credit history and Property semi urban.**

**Synthetic Minority Oversampling Technique(SMOTE)
SMOTE first selects a minority class instance x¹ at random and finds its k nearest minority class neighbors. The synthetic instance is then created by choosing one of the k nearest neighbors x² at random and connecting x¹ and x² to form a line segment in the feature space. The synthetic instances are generated as a convex combination of the two chosen instances x² and x¹.**

**From the correlation matrix we can see that the independent variables are not correlated with each other and hence can be useful for modeling.**
"""

plt.figure(figsize=(15,8))
sns.heatmap(X.corr(),annot=True)
plt.show()

from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline
from collections import Counter

#The numbers before SMOTE
num_before = dict(Counter(y))

#Performing SMOTE

#Define pipeline
over = SMOTE(sampling_strategy=0.8)
under = RandomUnderSampler(sampling_strategy=0.8)
steps = [('o', over), ('u', under)]
pipeline = Pipeline(steps=steps)

#Transforming the dataset
X_smote, y_smote = pipeline.fit_resample(X, y)


#Tthe numbers after SMOTE
num_after =dict(Counter(y_smote))

print(num_before, num_after)

print(X_smote)

X1 = pd.DataFrame(X_smote)
y1= pd.DataFrame(y_smote)

new_data = pd.concat([X1, y1], axis=1)
new_data.columns = ['Loan_Amount_Term', 'Credit_History', 'Prop_area_Semiurban','Loan_Status']
new_data.head()

X_new = new_data.drop(['Loan_Status'],1)
y_new = new_data['Loan_Status']

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(X_new,y_new,test_size=0.2,random_state=0)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
x_train = sc.fit_transform(x_train)
x_test = sc.transform(x_test)

from sklearn.ensemble import RandomForestClassifier

rfc = RandomForestClassifier()
rfc.fit(x_train,y_train)
y_pred = rfc.predict(x_test)
from sklearn import metrics
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import f1_score
from sklearn.model_selection import cross_val_score

print('confusion Matrix:\n',confusion_matrix(y_pred,y_test))
print('Accuracy of the model: ',metrics.accuracy_score(y_test, y_pred))
print('Precision of the model', metrics.precision_score(y_test,y_pred))
print('Recall of the model: ',metrics.recall_score(y_test,y_pred))
print("F1 Score of the model:",f1_score(y_pred,y_test))

"""**From the above model we can see that it gives good results and hence can be used for further predictions. The relevant variables are Loan amt term, credit history and Property semi urban.**"""